{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_excel('/Users/rii_beltz/Desktop/candidate_classifier/data/processed/processed_data.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обработка текста:\n",
    "- удаление лишних символов (пунктуация, спецсимволы)\n",
    "- приведение текста к нижнему регистру\n",
    "- токенизация\n",
    "- удаление стоп-слов\n",
    "- лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/rii_beltz/nltk_data', '/Users/rii_beltz/Desktop/candidate_classifier/venv/nltk_data', '/Users/rii_beltz/Desktop/candidate_classifier/venv/share/nltk_data', '/Users/rii_beltz/Desktop/candidate_classifier/venv/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n",
    "\n",
    "nltk.download('punkt', download_dir='/Users/rii_beltz/nltk_data')\n",
    "nltk.download('stopwords', download_dir='/Users/rii_beltz/nltk_data')\n",
    "nltk.data.path.append('/Users/rii_beltz/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены: ['This', 'is', 'a', 'simple', 'test', 'sentence', 'for', 'tokenization', 'and', 'stopwords', 'removal', '.']\n",
      "После удаления стоп-слов: ['simple', 'test', 'sentence', 'tokenization', 'stopwords', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Пример текста\n",
    "text = \"This is a simple test sentence for tokenization and stopwords removal.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # Загружаем стоп-слова\n",
    "tokens = word_tokenize(text)  # Токенизация\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  # Удаление стоп-слов\n",
    "\n",
    "print(\"Токены:\", tokens)\n",
    "print(\"После удаления стоп-слов:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Очистка текста\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Удаление лишних пробелов\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаление пунктуации\n",
    "    text = text.lower()  # Приведение к нижнему регистру\n",
    "\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('russian'))  # Загрузка стоп-слов\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_experience(text):\n",
    "    pattern = r\"(опыт работы (от|до)? ?\\d+ (год|лет|месяц|года|годы)?)|(без опыта работы)\"\n",
    "    matches = re.findall(pattern, text.lower())\n",
    "    return [match[0] for match in matches if match[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16 лет' 'вакансия жду' 'вакансия жду именно' 'вносить свои идеи'\n",
      " 'встречи собеседовании' 'гражданско правового'\n",
      " 'гражданско правового характера' 'график работы' 'данная вакансия'\n",
      " 'данная вакансия жду' 'договору гражданско'\n",
      " 'договору гражданско правового' 'доп мотивацию' 'доп мотивацию первые'\n",
      " 'жду именно' 'жду именно твой' 'заинтересовала данная'\n",
      " 'заинтересовала данная вакансия' 'заявки подключение'\n",
      " 'идеи реализации работе' 'именно твой' 'именно твой отклик'\n",
      " 'индивидуальный график' 'индивидуальный график работы'\n",
      " 'клиентов действующим тарифам' 'который сможешь расти' 'месяца работы'\n",
      " 'мобильная связь' 'мотивацию первые' 'мотивацию первые месяца'\n",
      " 'наставника который' 'опыта работы' 'отклик встречи'\n",
      " 'отклик встречи собеседовании' 'отчетность требования' 'оформлять заявки'\n",
      " 'первые месяца' 'первые месяца работы' 'подключение услуг'\n",
      " 'правового характера' 'свои идеи' 'свои идеи реализации'\n",
      " 'совмещение учебой' 'твой отклик' 'твой отклик встречи'\n",
      " 'трудоустройство договору' 'трудоустройство договору гражданско'\n",
      " 'уровня топ' 'услуги компании' 'шанс заработать столько']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Загрузить стоп-слова для русского языка\n",
    "stop_words_russian = stopwords.words('russian')\n",
    "\n",
    "# Инициализация CountVectorizer с пользовательским списком стоп-слов\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words=stop_words_russian, max_features=50)\n",
    "\n",
    "# Применение к столбцу с описанием вакансий\n",
    "ngrams = vectorizer.fit_transform(text_data['Описание вакансии'])\n",
    "ngrams_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Вывод найденных n-грамм\n",
    "print(ngrams_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e21b911a58c4f528013925260888f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1dbfffaff18412a93b194fa17366838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.decoder.version', 'model.encoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a543a116f8024379a1a176f951a0bccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a75be4b418e4aafa3e900eaae8dc9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb4fb737d044e2f9a0bc0d900702146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a230bc2e3b4fc884cd95add3a0b38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Обязанности: Продвигать услуги компании. Требования: Желание зарабатывать.', 'labels': ['обязанности', 'условия', 'требования'], 'scores': [0.5210112929344177, 0.25631389021873474, 0.22267486155033112]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Загрузка альтернативной модели\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Классификация текста\n",
    "categories = [\"обязанности\", \"требования\", \"условия\"]\n",
    "text = \"Обязанности: Продвигать услуги компании. Требования: Желание зарабатывать.\"\n",
    "result = classifier(text, categories)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Извлеченный опыт работы: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Функция для извлечения опыта работы с помощью регулярных выражений\n",
    "def extract_experience_with_regex(text):\n",
    "    # Регулярное выражение для поиска опыта работы\n",
    "    experience_patterns = [\n",
    "        r\"\\b(\\d+)\\s*(год[а-я]*)\",  # Например, \"2 года\"\n",
    "        r\"\\b(\\d+)\\s*(месяц[а-я]*)\",  # Например, \"6 месяцев\"\n",
    "        r\"без\\sопыта\\sработы\"  # Упоминание \"без опыта работы\"\n",
    "    ]\n",
    "    \n",
    "    experience_phrases = []\n",
    "    for pattern in experience_patterns:\n",
    "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            if isinstance(match, tuple):\n",
    "                experience_phrases.append(\" \".join(match).strip())\n",
    "            else:\n",
    "                experience_phrases.append(match.strip())\n",
    "    \n",
    "    return experience_phrases\n",
    "\n",
    "# Пример текста вакансии\n",
    "vacancy_text = \"\"\"\n",
    "'Что мы предлагаем:  Доход до 80000 ₽, состоит из оклада и ежемесячной премии по результатам работы; График работы 5/2; Оформление в штат с первого рабочего дня, расширенный соцпакет для опытных сотрудников: компенсация затрат на фитнес или медицинские услуги, дополнительные выплаты к отпуску, компенсация процентов по ипотечному кредиту; Оборудование и спецодежда предоставляются, компенсация за использование личного автомобиля-при наличии; Всестороннее развитие: корпоративный онлайн-университет, доступ к электронным библиотекам с широким выбором бизнес-литературы, компенсация профильного внешнего обучения; Компенсация сотовой связи и специальные условия на услуги компании для сотрудников, корпоративные скидки у компаний-партнеров.  Чем предстоит заниматься:  Обслуживание текущих абонентов (физических лиц) и устранение перебоев доступа к услугам компании: интернет, интерактивное ТВ, видеонаблюдение, умный дом; Ремонт линий связи и настройка оборудования, проверка работоспособности; Консультирование клиентов по работе оборудования и техническим вопросам; Ведение отчетности в мобильном приложении.  Что мы от вас ждем:  Готовность к разъездному и интенсивному характеру работы; Грамотная речь Быстрая обучаемость и стремление постоянно учиться новому; Опыт работы в телекоммуникациях (подключение услуг связи или выездная техническая поддержка) Будет вашим преимуществом.  Готовы рассмотреть кандидатов с опытом работы на вакансиях: техник, монтажник, электрик, электромонтёр, системный администратор, техник-слаботочник, выездной мастер, инженер-монтажник, сервисный инженер.'\n",
    "\"\"\"\n",
    "\n",
    "# Извлечение опыта работы\n",
    "experience = extract_experience_with_regex(vacancy_text)\n",
    "print(\"Извлеченный опыт работы:\", experience)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
