{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_excel('/Users/rii_beltz/Desktop/candidate_classifier/data/processed/processed_data.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "обработка текста:\n",
    "- удаление лишних символов (пунктуация, спецсимволы)\n",
    "- приведение текста к нижнему регистру\n",
    "- токенизация\n",
    "- удаление стоп-слов\n",
    "- лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/rii_beltz/nltk_data', '/Users/rii_beltz/.pyenv/versions/3.10.12/nltk_data', '/Users/rii_beltz/.pyenv/versions/3.10.12/share/nltk_data', '/Users/rii_beltz/.pyenv/versions/3.10.12/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n",
    "\n",
    "nltk.download('punkt', download_dir='/Users/rii_beltz/nltk_data')\n",
    "nltk.download('stopwords', download_dir='/Users/rii_beltz/nltk_data')\n",
    "nltk.data.path.append('/Users/rii_beltz/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/rii_beltz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены: ['This', 'is', 'a', 'simple', 'test', 'sentence', 'for', 'tokenization', 'and', 'stopwords', 'removal', '.']\n",
      "После удаления стоп-слов: ['simple', 'test', 'sentence', 'tokenization', 'stopwords', 'removal', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Пример текста\n",
    "text = \"This is a simple test sentence for tokenization and stopwords removal.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))  # Загружаем стоп-слова\n",
    "tokens = word_tokenize(text)  # Токенизация\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]  # Удаление стоп-слов\n",
    "\n",
    "print(\"Токены:\", tokens)\n",
    "print(\"После удаления стоп-слов:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# Загрузка русской модели spaCy\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# Функция для предобработки текста\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Приведение к нижнему регистру\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Удаление пунктуации\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Удаление лишних пробелов\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для извлечения опыта работы\n",
    "def extract_experience(text):\n",
    "    doc = nlp(text)\n",
    "    experience_phrases = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"QUANTITY\" and (\"год\" in ent.text or \"месяц\" in ent.text):\n",
    "            experience_phrases.append(ent.text)\n",
    "    return experience_phrases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Словарь ключевых навыков\n",
    "skills_keywords = [\"python\", \"sql\", \"управление\", \"анализ\", \"телекоммуникации\", \"коммуникация\"]\n",
    "\n",
    "# Функция для поиска навыков\n",
    "def extract_skills(text):\n",
    "    tokens = text.split()\n",
    "    extracted_skills = [word for word in tokens if word in skills_keywords]\n",
    "    return extracted_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для определения графика\n",
    "def extract_schedule(text):\n",
    "    schedules = [\"гибкий график\", \"удаленная работа\", \"полный рабочий день\", \"вахтовый метод\"]\n",
    "    found_schedules = [schedule for schedule in schedules if schedule in text]\n",
    "    return found_schedules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для поиска специфических требований\n",
    "def extract_requirements(text):\n",
    "    requirements = [\"готовность к командировкам\", \"знание английского языка\", \"умение работать в команде\"]\n",
    "    found_requirements = [req for req in requirements if req in text]\n",
    "    return found_requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Опыт работы: []\n",
      "Навыки: []\n",
      "График работы: ['гибкий график']\n",
      "Специфические требования: ['готовность к командировкам', 'знание английского языка']\n"
     ]
    }
   ],
   "source": [
    "# Пример текста вакансии\n",
    "vacancy_text = \"\"\"\n",
    "Рассматриваем кандидатов без опыта работы. Требования: Желателен опыт работы 2 года. \n",
    "Опыт работы в телекоммуникациях будет преимуществом. График работы: гибкий график. \n",
    "Необходимо знание английского языка и готовность к командировкам.\n",
    "\"\"\"\n",
    "\n",
    "# Обработка текста\n",
    "preprocessed_text = preprocess_text(vacancy_text)\n",
    "\n",
    "# Извлечение данных\n",
    "experience = extract_experience(preprocessed_text)\n",
    "skills = extract_skills(preprocessed_text)\n",
    "schedule = extract_schedule(preprocessed_text)\n",
    "requirements = extract_requirements(preprocessed_text)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Опыт работы:\", experience)\n",
    "print(\"Навыки:\", skills)\n",
    "print(\"График работы:\", schedule)\n",
    "print(\"Специфические требования:\", requirements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rii_beltz/.pyenv/versions/3.10.12/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n",
      "/Users/rii_beltz/.pyenv/versions/3.10.12/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1784: UserWarning: MPS: nonzero op is supported natively starting from macOS 14.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:361.)\n",
      "  sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Опыт работы 3 года в телекоммуникациях будет преимуществом. Рассматриваем кандидатов без опыта.', 'labels': ['Требуется опыт', 'Опыт не требуется', 'Желателен опыт'], 'scores': [0.6049758195877075, 0.2031954675912857, 0.19182875752449036]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Zero-shot классификатор\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "text = \"Опыт работы 3 года в телекоммуникациях будет преимуществом. Рассматриваем кандидатов без опыта.\"\n",
    "labels = [\"Требуется опыт\", \"Желателен опыт\", \"Опыт не требуется\"]\n",
    "\n",
    "result = classifier(text, labels)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ORG\n",
      "SQL ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Загрузка модели\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# Пример текста\n",
    "text = \"Опыт работы 2 года. Желательно знание Python и SQL. Гибкий график.\"\n",
    "\n",
    "# Анализ текста\n",
    "doc = nlp(text)\n",
    "\n",
    "# Вывод сущностей\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 21:54:09.134 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/datasets/wiki_ner_rus/wiki_ner_rus_dataset.tar.gz download because of matching hashes\n",
      "2025-01-15 21:54:13.37 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/rus_dream_entity_detection/wiki_ner_rus_bert.tar.gz download because of matching hashes\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchcrf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeeppavlov\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_model, configs\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Загрузка модели NER\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner_rus_bert_probas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Пример текста\u001b[39;00m\n\u001b[1;32m      7\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mМы ищем Python-разработчика с опытом работы от 3 лет. Гибкий график работы.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/deeppavlov/core/commands/infer.py:55\u001b[0m, in \u001b[0;36mbuild_model\u001b[0;34m(config, mode, load_trained, install, download)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m parameter for the \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m component, so \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_path\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m will not be renewed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m'\u001b[39m, component_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m))))\n\u001b[0;32m---> 55\u001b[0m component \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m component_config:\n\u001b[1;32m     58\u001b[0m     model\u001b[38;5;241m.\u001b[39m_components_dict[component_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m component\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/deeppavlov/core/common/params.py:92\u001b[0m, in \u001b[0;36mfrom_params\u001b[0;34m(params, mode, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(e)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m---> 92\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(obj):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# find the submodels params recursively\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     config_params \u001b[38;5;241m=\u001b[39m {k: _init_param(v, mode) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config_params\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/deeppavlov/core/common/registry.py:74\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not registered.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cls_from_str(name)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcls_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_REGISTRY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/deeppavlov/core/common/registry.py:42\u001b[0m, in \u001b[0;36mcls_from_str\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected class description in a `module.submodules:ClassName` form, but got `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m                       \u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m, cls_name)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/deeppavlov/models/torch_bert/torch_transformers_sequence_tagger.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeeppavlov\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeeppavlov\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchModel\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeeppavlov\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_bert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CRF\n\u001b[1;32m     29\u001b[0m log \u001b[38;5;241m=\u001b[39m getLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtoken_from_subtoken\u001b[39m(units: torch\u001b[38;5;241m.\u001b[39mTensor, mask: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/deeppavlov/models/torch_bert/crf.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchcrf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CRF \u001b[38;5;28;01mas\u001b[39;00m CRFbase\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCRF\u001b[39;00m(CRFbase):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Class with Conditional Random Field from PyTorch-CRF library\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m       with modified training function\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchcrf'"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "\n",
    "# Загрузка модели NER\n",
    "model = build_model(configs.ner.ner_rus_bert_probas, download=True)\n",
    "\n",
    "# Пример текста\n",
    "texts = [\"Мы ищем Python-разработчика с опытом работы от 3 лет. Гибкий график работы.\"]\n",
    "\n",
    "# Предсказания\n",
    "results = model(texts)\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
